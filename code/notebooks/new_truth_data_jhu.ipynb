{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymmwr as pm\n",
    "import datetime\n",
    "import warnings\n",
    "import io\n",
    "import requests\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some common functions\n",
    "def get_jhu_raw():\n",
    "    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\"\n",
    "#     types = {\n",
    "#         'FIPS': np.uint32\n",
    "        \n",
    "#     }\n",
    "    return pd.read_csv(url)\n",
    "df = get_jhu_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# drop unnecessary columns\n",
    "cols = ['UID', 'iso2', 'iso3', 'code3', 'Admin2']\n",
    "df = df.drop(df[cols], axis=1)\n",
    "\n",
    "state_agg = df.groupby(['Province_State']).sum()\n",
    "us_nat = df.groupby(['Country_Region']).sum()\n",
    "# display(df)\n",
    "df_truth = df.drop(columns=df.columns[list(range(1,7))])\n",
    "df_state_nat = state_agg.append(us_nat)\n",
    "# display(df_truth)\n",
    "df_truth_cumulative = df_truth\n",
    "df_truth_incident = df_truth[df_truth.columns[1:]] - df_truth[df_truth.columns[1:]].shift(periods=1, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['US', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "              'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky',\n",
    "              'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',\n",
    "              'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
    "              'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island',\n",
    "              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
    "              'West Virginia', 'Wisconsin', 'Wyoming', 'District of Columbia']\n",
    "df_orig = pd.read_csv(url)\n",
    "df_orig = df_orig[df_orig['Province_State'].isin(states)]\n",
    "# print(df_orig.columns[:30])\n",
    "df_orig = df_orig[pd.notna(df_orig['FIPS'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JHU truth data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "fips_codes = pd.read_csv('../../data-locations/locations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epi_data(date):\n",
    "    format_str = '%m/%d/%y'  # The format\n",
    "    dt = datetime.datetime.strptime(date, format_str).date()\n",
    "    epi = pm.date_to_epiweek(dt)\n",
    "    return epi.year, epi.week, epi.day\n",
    "\n",
    "def configure_JHU_data(df, target):\n",
    "    # convert matrix to repeating row format\n",
    "    df_truth = df.unstack()\n",
    "    df_truth = df_truth.reset_index()\n",
    "\n",
    "    # get epi data from date\n",
    "    df_truth['year'], df_truth['week'], df_truth['day'] = \\\n",
    "        zip(*df_truth['level_0'].map(get_epi_data))\n",
    "\n",
    "    # rename columns\n",
    "    df_truth = df_truth.rename(columns={0: \"value\",\n",
    "                                        \"level_1\": \"location_long\"})\n",
    "\n",
    "    # Get state IDs\n",
    "    df_truth = df_truth.merge(fips_codes, left_on='location_long', right_on='location_name', how='left')\n",
    "\n",
    "    # Drop NAs\n",
    "    df_truth = df_truth.dropna(subset=['location', 'value'])\n",
    "\n",
    "    # add leading zeros to state code\n",
    "    df_truth['location'] = df_truth['location'].apply(lambda x: '{0:0>2}'.format(x))\n",
    "\n",
    "    '''\n",
    "    ####################################\n",
    "    # Daily truth data output for reference\n",
    "    ####################################\n",
    "    '''\n",
    "\n",
    "    # only output \"location\", \"epiweek\", \"value\"\n",
    "    df_truth = df_truth.drop(['location_name'], axis=1)\n",
    "    df_byday = df_truth.rename(columns={\"level_0\": \"date\", \"location_long\": \"location_name\"})\n",
    "\n",
    "    # select columns\n",
    "    df_byday = df_byday[[\"date\", \"location\", \"location_name\", \"value\"]]\n",
    "\n",
    "    # ensure value column is integer\n",
    "    df_byday['value'] = df_byday['value'].astype(int)\n",
    "\n",
    "    # change to yyyy/mm/dd format\n",
    "    df_byday['date'] = pd.to_datetime(df_byday['date'])\n",
    "\n",
    "    file_path = '../../data-truth/truth-' + target + '.csv'\n",
    "    df_byday.to_csv(file_path, index=False)\n",
    "\n",
    "    '''\n",
    "    ####################################\n",
    "    # Truth data output for visualization\n",
    "    ####################################\n",
    "    '''\n",
    "    # Only visualize certain states\n",
    "    states = ['US', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "              'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky',\n",
    "              'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',\n",
    "              'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
    "              'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island',\n",
    "              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
    "              'West Virginia', 'Wisconsin', 'Wyoming', 'District of Columbia']\n",
    "    df_truth = df_truth[df_truth[\"location_long\"].isin(states)]\n",
    "\n",
    "    # Observed data on the seventh day\n",
    "    # or group by week for incident deaths\n",
    "    if target == 'Incident Deaths':\n",
    "        df_vis = df_truth.groupby(['week', 'location_long'], as_index=False).agg({'level_0': 'last',\n",
    "                                                                                  'value': 'sum',\n",
    "                                                                                  'year': 'last',\n",
    "                                                                                  'day': 'last',\n",
    "                                                                                  'location': 'last',\n",
    "                                                                                  'abbreviation': 'last'})\n",
    "        df_vis = df_vis[df_vis['day'] == 7]\n",
    "    else:\n",
    "        df_vis = df_truth[df_truth['day'] == 7]\n",
    "\n",
    "    df_vis['week'] = df_vis['week'] + 1  # shift epiweek on axis\n",
    "\n",
    "    # add leading zeros to epi week\n",
    "    df_vis['week'] = df_vis['week'].apply(lambda x: '{0:0>2}'.format(x))\n",
    "\n",
    "    # define epiweek\n",
    "    df_vis['epiweek'] = df_vis['year'].astype(str) + df_vis['week']\n",
    "\n",
    "    # Replace US with \"nat\" this is NECESSARY for visualization code!\n",
    "    df_vis.loc[df_vis[\"location_long\"] == \"US\", \"abbreviation\"] = \"nat\"\n",
    "\n",
    "    # only output \"location\", \"epiweek\", \"value\"\n",
    "    df_truth_short = df_vis[[\"abbreviation\", \"epiweek\", \"value\"]]\n",
    "    df_truth_short = df_truth_short.rename(columns={\"abbreviation\": \"location\"})\n",
    "\n",
    "    df_truth_short[\"value\"].replace({0: 0.1}, inplace=True)\n",
    "\n",
    "    file_path = '../../visualization/vis-master/covid-csv-tools/dist/truth/' + target + '.json'\n",
    "    # write to json\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(df_truth_short.to_json(orient='records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_truth_cumulative)\n",
    "configure_JHU_data(df_truth_cumulative.drop(columns=['FIPS']), \"Cumulative Deaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_way():\n",
    "    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\"\n",
    "    url_req = requests.get(url).content\n",
    "    df = pd.read_csv(io.StringIO(url_req.decode('utf-8')))\n",
    "\n",
    "    fips_codes = pd.read_csv('../../data-locations/locations.csv')\n",
    "\n",
    "    \n",
    "    # aggregate by state and nationally\n",
    "    state_agg = df.groupby(['Province_State']).sum()\n",
    "    us_nat = df.groupby(['Country_Region']).sum()\n",
    "    df_state_nat = state_agg.append(us_nat)\n",
    "\n",
    "    # drop unnecessary columns\n",
    "    cols = list(range(0, 6))\n",
    "    df_truth = df_state_nat.drop(df_state_nat.columns[cols], axis=1)\n",
    "\n",
    "    df_truth_cumulative = df_truth\n",
    "    df_truth_incident = df_truth - df_truth.shift(periods=1, axis='columns')\n",
    "\n",
    "    display(df_truth_cumulative)\n",
    "    configure_JHU_data(df_truth_cumulative, \"Cumulative Deaths\")\n",
    "    configure_JHU_data(df_truth_incident, \"Incident Deaths\")\n",
    "    return df_truth_cumulative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth_states = old_way()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New way with county level forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_jhu(df):\n",
    "    cols_to_drop = ['UID', 'iso2', 'iso3', 'code3', 'Admin2', 'Country_Region', \n",
    "                    'Lat', 'Long_', 'Combined_Key', 'Population']\n",
    "    df_final = df.drop(df[df['FIPS'].isna()].index)\n",
    "\n",
    "    df_final.drop(columns=cols_to_drop, inplace=True)\n",
    "    df_final['FIPS'] = df_final['FIPS'].astype(np.int32)\n",
    "    fips_codes = pd.read_csv('../../data-locations/locations.csv')\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "df = preprocess_raw_jhu(get_jhu_raw())\n",
    "df_truth = df.drop(columns=['Province_State']).set_index(['FIPS'])\n",
    "\n",
    "state_fips = fips_codes[fips_codes['abbreviation'].notna()]\n",
    "df_s_merged = df_truth_states.merge(state_fips, left_index=True, right_on='location_name', how='left')\n",
    "df_states = df_s_merged.loc[df_s_merged.index.dropna()]\n",
    "df_states.index = df_states.index.astype(np.int64)\n",
    "df_states.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_states.columns))\n",
    "display(df_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
