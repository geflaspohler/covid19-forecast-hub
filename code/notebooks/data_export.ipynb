{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymmwr as pm\n",
    "import datetime\n",
    "import warnings\n",
    "import io\n",
    "import requests\n",
    "warnings.simplefilter(action='ignore')\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some common functions\n",
    "def get_jhu_raw():\n",
    "    url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\"\n",
    "    return pd.read_csv(url)\n",
    "\n",
    "# Read in fips codes and raw health data\n",
    "fips_codes = pd.read_csv('../../data-locations/locations.csv')\n",
    "df = get_jhu_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epi_data(date):\n",
    "    ''' Get date time in epi fromat'''\n",
    "    format_str = '%m/%d/%y'  # The format\n",
    "    dt = datetime.datetime.strptime(date, format_str).date()\n",
    "    epi = pm.date_to_epiweek(dt)\n",
    "    return epi.year, epi.week, epi.day\n",
    "\n",
    "def configure_JHU_data(df, target):\n",
    "    # convert matrix to repeating row format\n",
    "    df_truth = df.unstack()\n",
    "    df_truth = df_truth.reset_index()\n",
    "\n",
    "    # get epi data from date\n",
    "    df_truth['year'], df_truth['week'], df_truth['day'] = \\\n",
    "        zip(*df_truth['level_0'].map(get_epi_data))\n",
    "\n",
    "    # rename columns\n",
    "    df_truth = df_truth.rename(columns={0: \"value\",\n",
    "                                        \"level_1\": \"location_long\"})\n",
    "\n",
    "    # Get state IDs\n",
    "    df_truth = df_truth.merge(fips_codes, left_on='location_long', right_on='location_name', how='left')\n",
    "\n",
    "    # Drop NAs\n",
    "    df_truth = df_truth.dropna(subset=['location', 'value'])\n",
    "\n",
    "    # add leading zeros to state code\n",
    "    df_truth['location'] = df_truth['location'].apply(lambda x: '{0:0>2}'.format(x))\n",
    "\n",
    "    '''\n",
    "    ####################################\n",
    "    # Daily truth data output for reference\n",
    "    ####################################\n",
    "    '''\n",
    "\n",
    "    # only output \"location\", \"epiweek\", \"value\"\n",
    "    df_truth = df_truth.drop(['location_name'], axis=1)\n",
    "    df_byday = df_truth.rename(columns={\"level_0\": \"date\", \"location_long\": \"location_name\"})\n",
    "\n",
    "    # select columns\n",
    "    df_byday = df_byday[[\"date\", \"location\", \"location_name\", \"value\"]]\n",
    "\n",
    "    # ensure value column is integer\n",
    "    df_byday['value'] = df_byday['value'].astype(int)\n",
    "\n",
    "    # change to yyyy/mm/dd format\n",
    "    df_byday['date'] = pd.to_datetime(df_byday['date'])\n",
    "\n",
    "    file_path = '../../data-truth/truth-' + target + '.csv'\n",
    "    df_byday.to_csv(file_path, index=False)\n",
    "\n",
    "    '''\n",
    "    ####################################\n",
    "    # Truth data output for visualization\n",
    "    ####################################\n",
    "    '''\n",
    "    # Only visualize certain states\n",
    "    states = ['US', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "              'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky',\n",
    "              'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',\n",
    "              'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
    "              'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island',\n",
    "              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
    "              'West Virginia', 'Wisconsin', 'Wyoming', 'District of Columbia']\n",
    "    df_truth = df_truth[df_truth[\"location_long\"].isin(states)]\n",
    "\n",
    "    # Observed data on the seventh day\n",
    "    # or group by week for incident deaths\n",
    "    if target == 'Incident Deaths':\n",
    "        df_vis = df_truth.groupby(['week', 'location_long'], as_index=False).agg({'level_0': 'last',\n",
    "                                                                                  'value': 'sum',\n",
    "                                                                                  'year': 'last',\n",
    "                                                                                  'day': 'last',\n",
    "                                                                                  'location': 'last',\n",
    "                                                                                  'abbreviation': 'last'})\n",
    "        df_vis = df_vis[df_vis['day'] == 7]\n",
    "    else:\n",
    "        df_vis = df_truth[df_truth['day'] == 7]\n",
    "\n",
    "    df_vis['week'] = df_vis['week'] + 1  # shift epiweek on axis\n",
    "\n",
    "    # add leading zeros to epi week\n",
    "    df_vis['week'] = df_vis['week'].apply(lambda x: '{0:0>2}'.format(x))\n",
    "\n",
    "    # define epiweek\n",
    "    df_vis['epiweek'] = df_vis['year'].astype(str) + df_vis['week']\n",
    "\n",
    "    # Replace US with \"nat\" this is NECESSARY for visualization code!\n",
    "    df_vis.loc[df_vis[\"location_long\"] == \"US\", \"abbreviation\"] = \"nat\"\n",
    "\n",
    "    # only output \"location\", \"epiweek\", \"value\"\n",
    "    df_truth_short = df_vis[[\"abbreviation\", \"epiweek\", \"value\"]]\n",
    "    df_truth_short = df_truth_short.rename(columns={\"abbreviation\": \"location\"})\n",
    "\n",
    "    df_truth_short[\"value\"].replace({0: 0.1}, inplace=True)\n",
    "\n",
    "    file_path = '../../visualization/vis-master/covid-csv-tools/dist/truth/' + target + '.json'\n",
    "    # write to json\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(df_truth_short.to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare ground truth data, incident and cummulative deaths, daily \n",
    "\"\"\"\n",
    "# aggregate by state and nationally\n",
    "state_agg = df.groupby(['Province_State']).sum()\n",
    "us_nat = df.groupby(['Country_Region']).sum()\n",
    "df_state_nat = state_agg.append(us_nat)\n",
    "\n",
    "drop_cols = ['UID', 'code3', 'FIPS', 'Lat', 'Long_', 'Population']\n",
    "# drop unnecessary columns\n",
    "df_truth = df_state_nat.drop(columns=drop_cols, axis=1)\n",
    "\n",
    "df_truth_cumulative = df_truth\n",
    "df_truth_incident = df_truth - df_truth.shift(periods=1, axis='columns')\n",
    "\n",
    "configure_JHU_data(df_truth_cumulative, \"Cumulative Deaths\")\n",
    "configure_JHU_data(df_truth_incident, \"Incident Deaths\")\n",
    "\n",
    "state_fips = fips_codes[fips_codes['abbreviation'].notna()]\n",
    "df_s_merged = df_truth_cumulative.merge(state_fips, left_index=True, right_on='location_name', how='left')\n",
    "df_states = df_s_merged.loc[df_s_merged.index.dropna()]\n",
    "df_states.index = df_states.index.astype(np.int64)\n",
    "df_states.sort_index(inplace=True)\n",
    "\n",
    "date_columns = df_states.columns[0:-4]\n",
    "df_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display cummulative deaths by state and overall\n",
    "\"\"\"\n",
    "df_plot = df_states.set_index('abbreviation')\n",
    "df_plot = df_plot[date_columns].T\n",
    "\n",
    "ax = df_plot.plot(kind='line')\n",
    "ax.legend(loc=(1.01, 0.01), ncol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Store model prediciotns, organized by dates (can be slow, run as little as possible)\n",
    "\"\"\"\n",
    "\n",
    "# Skip over certain models that should not be compared\n",
    "models_to_exclude = [\"\"]\n",
    "\n",
    "model_predictions = {}\n",
    "state_fips = fips_codes[fips_codes['abbreviation'].notna()] # State FIPS codes\n",
    "\n",
    "template = os.path.join('..', '..', 'data-processed','*')\n",
    "# Number of directories above model name\n",
    "model_name_index = 3\n",
    "for model_dir in glob.glob(template):\n",
    "    mn = model_dir.split(os.path.sep)[model_name_index]\n",
    "    if mn in models_to_exclude:\n",
    "        continue\n",
    "    \n",
    "    file_template = os.path.join(model_dir, f'*-{mn}.csv')\n",
    "    for fh in glob.glob(file_template):\n",
    "        df = pd.read_csv(fh)\n",
    "        # df = df.merge(state_fips, left_index=True, right_on='location_name', how='left')\n",
    "\n",
    "        df['location'] = df['location'].astype(str).str.lstrip(\"0\")\n",
    "    \n",
    "#         pdb.set_trace()\n",
    "        forecast_date = df['target_end_date'][0]\n",
    "#         forecast_date = df['forecast_date'][0]\n",
    "\n",
    "        # Use code -1 for point tasks\n",
    "        df.loc[df['type'] == \"point\", 'quantile'] = -1\n",
    "\n",
    "        task_df = pd.pivot_table(df, index=['target', 'location', 'quantile']).rename(columns={\"value\": f\"{mn}\"})\n",
    "        \n",
    "        if forecast_date not in model_predictions:\n",
    "            model_predictions[forecast_date] = task_df\n",
    "        else:\n",
    "            model_predictions[forecast_date] = model_predictions[forecast_date].merge(\n",
    "                task_df, how='outer', left_index=True, right_index=True)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get list of all task, quantiles, date, model, locations combiations in the dataset  \n",
    "\"\"\"\n",
    "# Display all possible tasks\n",
    "all_dates = list(model_predictions.keys())\n",
    "all_dates.sort()\n",
    "\n",
    "all_locations = list(state_fips['location'].str.lstrip(\"0\"))\n",
    "\n",
    "all_models = []\n",
    "for date in model_predictions.keys():\n",
    "    all_models = all_models + model_predictions[date].columns.tolist()\n",
    "all_models = list(set(all_models))\n",
    "\n",
    "# Populate targets and quantiles present in the dataset \n",
    "all_quantiles = None\n",
    "all_targets = None\n",
    "for key, value in model_predictions.items():\n",
    "    # Update targets\n",
    "    if all_targets is None:\n",
    "        all_targets = value.index.get_level_values(\"target\").unique()\n",
    "    else:\n",
    "        all_targets = all_targets.union(value.index.get_level_values(\"target\")).unique()\n",
    "        \n",
    "    # Update quantiles \n",
    "    if all_quantiles is None:\n",
    "        all_quantiles = value.index.get_level_values(\"quantile\").unique()\n",
    "    else:\n",
    "        all_quantiles = all_quantiles.union(value.index.get_level_values(\"quantile\")).unique()        \n",
    "\n",
    "# Convert to lists \n",
    "all_targets = list(all_targets)\n",
    "all_quantiles = list(all_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize dataframe for counting avaliable data, both by task/model and task/date paris\n",
    "\"\"\"\n",
    "# Set up target, location, quantile index for experiments\n",
    "quantile = [0.5]\n",
    "experiments_ind = pd.MultiIndex.from_product([all_targets, all_locations, quantile], names=['target', 'location', 'quantile'])\n",
    "\n",
    "# Initialize experimental dataframes\n",
    "experiments_by_model = pd.DataFrame(index=experiments_ind, columns=all_models) \n",
    "experiments_by_date = pd.DataFrame(index=experiments_ind, columns=all_dates)\n",
    "experiments_by_model = experiments_by_model.fillna(0)\n",
    "experiments_by_date = experiments_by_date.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_abbrev = state_fips\n",
    "experiments_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Populate the number of present predictions, both by date and by model (can be slow, run as little as possible)\n",
    "\"\"\"\n",
    "for date in model_predictions.keys():\n",
    "    sub_df = model_predictions[date][model_predictions[date].index.get_level_values(\"quantile\") == quantile].notna()\n",
    "    experiments_by_model = experiments_by_model.add(sub_df, fill_value=0)\n",
    "    experiments_by_date[date].add(sub_df.sum(axis=1), fill_value=0)\n",
    "    experiments_by_date[date].add(sub_df.sum(axis=1), fill_value=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename location codes to human readable abbreviations\n",
    "state_abbrev = dict(zip(state_fips['location'].str.strip(\"0\"), state_fips['abbreviation']))\n",
    "experiments_by_model = experiments_by_model.rename(index=state_abbrev)\n",
    "experiments_by_date = experiments_by_date.rename(index=state_abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count the number of dates for each task in which we have at least N models making predictions \n",
    "\"\"\"\n",
    "N = 5\n",
    "n_dates = (experiments_by_date >= N).sum(axis=1)\n",
    "df_withn = copy.copy(experiments_by_model).rename(index=state_abbrev)\n",
    "df_withn[f'geq{N}'] = n_dates\n",
    "df_withn = df_withn[[f'geq{N}'] + list(df_withn.columns[0:-2])] # reorder columns so geqN is near front \n",
    "df_withn_sorted = df_withn.sort_values(by=f\"geq{N}\", ascending=False)\n",
    "print(\"Total number of tasks/locations/models:\", df_withn_sorted.shape)\n",
    "print(\"Max number of dates with N models for any task:\", (experiments_by_date > N).sum(axis=1).max())\n",
    "\n",
    "\"\"\"\n",
    "Display and write to csv file the sorted value\n",
    "\"\"\"\n",
    "df_withn_sorted.to_csv(f\"./sort-top{N}-q{quantile}.csv\", sep=\",\")\n",
    "df_withn_sorted.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query avaliability for a specific task\n",
    "\"\"\"\n",
    "task = '1 wk ahead cum death'\n",
    "quantile = 0.5\n",
    "location = 'US'\n",
    "\n",
    "df_experiment = pd.DataFrame(index=all_models, columns=all_dates)\n",
    "df_experiment = df_experiment.fillna(0)\n",
    "for date in model_predictions.keys():\n",
    "    sub_df = model_predictions[date].query(f\"target == '{task}' and quantile == '{quantile}' and location == '{location}'\").notna()\n",
    "    if sub_df.shape[0] != 1: \n",
    "        print(f\"Warning: {sub_df.shape[0]} model predictions found for task on date {date}\")\n",
    "        continue\n",
    "    df_experiment[date] = df_experiment[date].add(sub_df.iloc[0], fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot data avaliblity as number of dates with >= N dates avalible\n",
    "\"\"\"\n",
    "n_avail = []\n",
    "for n in range(1, 30):\n",
    "    n_avail.append((df_experiment.sum(axis=0) >= n).sum())\n",
    "plt.plot(range(1,30), n_avail)\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Number Models')\n",
    "\n",
    "print((df_experiment.sum(axis=0) >= 4).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Disaply and write avaliblity to file\n",
    "\"\"\"\n",
    "print(\"Avalilbity by date\")\n",
    "display(df_experiment.sum(axis=0))\n",
    "print(\"Avalilbity by model\")\n",
    "display(df_experiment.sum(axis=1).sort_values(ascending=False))\n",
    "\n",
    "# Sort models/rows by the number of dates avaliable\n",
    "row_index = df_experiment.sum(axis=1).sort_values(ascending=False).index\n",
    "df_experiment_sort = df_experiment.loc[row_index, :]\n",
    "\n",
    "df_experiment_sort['total_dates'] = df_experiment_sort.sum(axis=1)\n",
    "df_experiment_sort.loc['total_models'] = df_experiment_sort.sum(axis=0)\n",
    "df_experiment_sort = df_experiment_sort.loc[['total_models'] + list(df_experiment_sort.index[0:-1]), ['total_dates'] + list(df_experiment.columns)]\n",
    "\n",
    "print(\"Model/Date prediction avaliblity\")\n",
    "display(df_experiment_sort)\n",
    "\n",
    "df_experiment_sort.to_csv(f\"./overall-task{''.join(task.split())}-q{quantile}-location{location}.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # experiments_by_model.astype('float').nlargest(10, columns=experiments_by_model.columns)\n",
    "# rows_index=experiments_by_model.max(axis=1).sort_values(ascending=False).index\n",
    "# col_index=experiments_by_model.max().sort_values(ascending=False).index\n",
    "# new_df=experiments_by_model.loc[rows_index,col_index].rename(index=state_abbrev)\n",
    "# new_df.to_csv(f\"~/Documents/max-q{quantile[0]}.csv\", sep=\",\")\n",
    "# new_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_experiment_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
